{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMh6LVzex73j",
        "outputId": "8569132d-bcea-4f9d-c723-e5a7dba7d91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to load 2 CSV files...\n",
            "Loaded successfully: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "Loaded successfully: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "\n",
            "All files merged. Total dataset shape: (396111, 79)\n",
            "\n",
            "Starting SMOTE for imbalance handling...\n",
            "SMOTE complete successfully!\n",
            "New training shape: (425178, 78)\n",
            "\n",
            "\n",
            "--- Training Random Forest Model ---\n",
            "\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     53148\n",
            "           1       1.00      1.00      1.00     26041\n",
            "\n",
            "    accuracy                           1.00     79189\n",
            "   macro avg       1.00      1.00      1.00     79189\n",
            "weighted avg       1.00      1.00      1.00     79189\n",
            "\n",
            "Random Forest Accuracy: 0.9997\n",
            "\n",
            "Confusion Matrix:\n",
            "[[53148     0]\n",
            " [   24 26017]]\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 0. SETUP AND LIBRARY IMPORTS\n",
        "# ----------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. DATA LOADING AND CONCATENATION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# CRITICAL STEP: Ensure this path is 100% correct.\n",
        "# If your folder name is different, change it here:\n",
        "base_path = '/content/drive/MyDrive/CICIDS2017/'\n",
        "\n",
        "# --- AUTOMATIC FILE DISCOVERY ---\n",
        "# This finds ALL files ending in .csv (or .CSV) inside the directory.\n",
        "file_paths = glob.glob(base_path + '*.csv')\n",
        "# Add search for common capital extension if necessary (optional but safe)\n",
        "file_paths.extend(glob.glob(base_path + '*.CSV'))\n",
        "\n",
        "if not file_paths:\n",
        "    raise ValueError(f\"CRITICAL ERROR: No CSV files found in the directory. Path: {base_path}\")\n",
        "\n",
        "data_frames = []\n",
        "print(f\"Starting to load {len(file_paths)} CSV files...\")\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        df_temp = pd.read_csv(file_path, low_memory=False)\n",
        "        data_frames.append(df_temp)\n",
        "        print(f\"Loaded successfully: {file_path.split('/')[-1]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL ERROR during reading: {file_path.split('/')[-1]}. Error: {e}\")\n",
        "\n",
        "if not data_frames:\n",
        "    raise ValueError(\"Cannot proceed. Data loading failed after discovery. Check file integrity or permissions.\")\n",
        "\n",
        "df = pd.concat(data_frames, ignore_index=True)\n",
        "print(f\"\\nAll files merged. Total dataset shape: {df.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. CLEANING AND SCALING (DATA PREPARATION PIPELINE)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# 2.1 Clean Column Names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# 2.2 Drop Irrelevant/ID Columns\n",
        "cols_to_drop = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Unnamed: 0']\n",
        "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
        "\n",
        "# 2.3 Handle NaN and Infinite values\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 2.4 Handle Target Label (Binary Classification: 0=Normal, 1=Attack)\n",
        "df['Label'] = df['Label'].apply(lambda x: 0 if str(x).strip() == 'BENIGN' else 1)\n",
        "Y = df['Label']\n",
        "X = df.drop(columns=['Label'])\n",
        "\n",
        "# 2.5 Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# 2.6 Data Split (80% Train, 20% Test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42, stratify=Y)\n",
        "\n",
        "# 2.7 Handle Imbalance with SMOTE on Training Data\n",
        "print(\"\\nStarting SMOTE for imbalance handling...\")\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Balance ONLY minority classes → avoid RAM crash\n",
        "sm = SMOTE(random_state=42, sampling_strategy='not majority')\n",
        "\n",
        "X_train_res, Y_train_res = sm.fit_resample(X_train, Y_train)\n",
        "\n",
        "print(\"SMOTE complete successfully!\")\n",
        "print(f\"New training shape: {X_train_res.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. MODEL 1: RANDOM FOREST IMPLEMENTATION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\\n--- Training Random Forest Model ---\")\n",
        "\n",
        "# A. Initialization and Training\n",
        "# n_jobs=-1 uses all available cores for faster training\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=25, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train_res, Y_train_res)\n",
        "\n",
        "\n",
        "# B. Testing and Prediction\n",
        "Y_pred_rf = rf_model.predict(X_test)\n",
        "# Save probabilities for the Hybrid Model (Task 3)\n",
        "Y_proba_rf = rf_model.predict_proba(X_test)\n",
        "\n",
        "# C. Evaluation\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(Y_test, Y_pred_rf))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(Y_test, Y_pred_rf):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, Y_pred_rf))\n",
        "\n",
        "rf_metrics = classification_report(Y_test, Y_pred_rf, output_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 4. MODEL 2: DENSE NEURAL NETWORK (DL Implementation)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# NOTE: This code assumes X_train and Y_train are defined from Sections 1, 2, 3.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\\n--- 4. Training Dense Neural Network Model ---\")\n",
        "\n",
        "# We use the original training variables (X_train, Y_train) to avoid memory issues\n",
        "# that SMOTE caused, and apply class weighting for imbalance handling.\n",
        "X_train_data = X_train\n",
        "Y_train_data = Y_train\n",
        "\n",
        "# 1. Calculate Class Weights for Imbalance\n",
        "# This tells the model to pay more attention to the minority (Attack) class.\n",
        "classes = np.unique(Y_train_data)\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=Y_train_data\n",
        ")\n",
        "class_weights_dict = dict(zip(classes, weights))\n",
        "print(f\"Calculated Class Weights: {class_weights_dict}\")\n",
        "\n",
        "\n",
        "# 2. Build the DNN Architecture\n",
        "input_dim = X_train_data.shape[1]\n",
        "num_classes = 1 # Binary classification output\n",
        "\n",
        "dnn_model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(input_dim,)),\n",
        "    Dropout(0.2),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(num_classes, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# 3. Compile the Model\n",
        "dnn_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "# 4. Training (This may take a few minutes)\n",
        "print(\"Starting DNN training (using class weights for balance)...\")\n",
        "history = dnn_model.fit(X_train_data, Y_train_data,\n",
        "                        epochs=10, # Number of training iterations\n",
        "                        batch_size=32,\n",
        "                        validation_split=0.1,\n",
        "                        class_weight=class_weights_dict, # Apply class weights here\n",
        "                        verbose=1)\n",
        "\n",
        "# 5. Testing and Prediction\n",
        "Y_pred_proba_dnn = dnn_model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to classes (0 or 1)\n",
        "Y_pred_dnn = (Y_pred_proba_dnn > 0.5).astype(int)\n",
        "\n",
        "# Get probabilities for Hybrid Model (Task 3)\n",
        "# Y_proba_dnn is the confidence for both classes (Normal and Attack)\n",
        "Y_proba_dnn = np.concatenate([1 - Y_pred_proba_dnn, Y_pred_proba_dnn], axis=1)\n",
        "\n",
        "# 6. Evaluation\n",
        "print(\"\\nDNN Classification Report:\")\n",
        "print(classification_report(Y_test, Y_pred_dnn))\n",
        "print(f\"DNN Accuracy: {accuracy_score(Y_test, Y_pred_dnn):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, Y_pred_dnn))\n",
        "\n",
        "dnn_metrics = classification_report(Y_test, Y_pred_dnn, output_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmDtUp5Uehg9",
        "outputId": "eda64316-31e4-4a01-f11a-8fa74641cf07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- 4. Training Dense Neural Network Model ---\n",
            "Calculated Class Weights: {np.int64(0): np.float64(0.744989157482278), np.int64(1): np.float64(1.5204533236050843)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DNN training (using class weights for balance)...\n",
            "Epoch 1/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 7ms/step - accuracy: 0.9856 - loss: 0.0421 - precision: 0.9710 - recall: 0.9853 - val_accuracy: 0.9904 - val_loss: 0.0210 - val_precision: 0.9749 - val_recall: 0.9964\n",
            "Epoch 2/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0197 - precision: 0.9847 - recall: 0.9943 - val_accuracy: 0.9970 - val_loss: 0.0138 - val_precision: 0.9928 - val_recall: 0.9982\n",
            "Epoch 3/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - accuracy: 0.9936 - loss: 0.0176 - precision: 0.9854 - recall: 0.9953 - val_accuracy: 0.9817 - val_loss: 0.0634 - val_precision: 0.9489 - val_recall: 0.9981\n",
            "Epoch 4/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 7ms/step - accuracy: 0.9944 - loss: 0.0156 - precision: 0.9865 - recall: 0.9966 - val_accuracy: 0.9973 - val_loss: 0.0136 - val_precision: 0.9928 - val_recall: 0.9989\n",
            "Epoch 5/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0150 - precision: 0.9875 - recall: 0.9969 - val_accuracy: 0.9968 - val_loss: 0.0138 - val_precision: 0.9920 - val_recall: 0.9985\n",
            "Epoch 6/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 7ms/step - accuracy: 0.9952 - loss: 0.0137 - precision: 0.9885 - recall: 0.9971 - val_accuracy: 0.9969 - val_loss: 0.0217 - val_precision: 0.9914 - val_recall: 0.9992\n",
            "Epoch 7/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - accuracy: 0.9955 - loss: 0.0131 - precision: 0.9888 - recall: 0.9977 - val_accuracy: 0.9977 - val_loss: 0.0137 - val_precision: 0.9939 - val_recall: 0.9990\n",
            "Epoch 8/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - accuracy: 0.9958 - loss: 0.0124 - precision: 0.9896 - recall: 0.9976 - val_accuracy: 0.9975 - val_loss: 0.0284 - val_precision: 0.9936 - val_recall: 0.9988\n",
            "Epoch 9/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - accuracy: 0.9954 - loss: 0.0138 - precision: 0.9887 - recall: 0.9976 - val_accuracy: 0.9976 - val_loss: 0.0218 - val_precision: 0.9938 - val_recall: 0.9990\n",
            "Epoch 10/10\n",
            "\u001b[1m8909/8909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0131 - precision: 0.9900 - recall: 0.9979 - val_accuracy: 0.9975 - val_loss: 0.0219 - val_precision: 0.9934 - val_recall: 0.9990\n",
            "\u001b[1m2475/2475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
            "\n",
            "DNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     53148\n",
            "           1       0.99      1.00      1.00     26041\n",
            "\n",
            "    accuracy                           1.00     79189\n",
            "   macro avg       1.00      1.00      1.00     79189\n",
            "weighted avg       1.00      1.00      1.00     79189\n",
            "\n",
            "DNN Accuracy: 0.9978\n",
            "\n",
            "Confusion Matrix:\n",
            "[[52995   153]\n",
            " [   25 26016]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hybrid Model**"
      ],
      "metadata": {
        "id": "DiHS-L4ltdye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 5. HYBRID MODEL DEVELOPMENT (Stacked Generalization)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\\n--- 5. Training Hybrid (Stacked) Model ---\")\n",
        "\n",
        "# --- Step 1: Create the new training feature set for the Meta-Classifier ---\n",
        "# Get training predictions from Random Forest\n",
        "Y_proba_rf_train = rf_model.predict_proba(X_train)\n",
        "# Get training predictions from DNN\n",
        "Y_proba_dnn_train = dnn_model.predict(X_train)\n",
        "Y_proba_dnn_train = np.concatenate([1 - Y_proba_dnn_train, Y_proba_dnn_train], axis=1)\n",
        "\n",
        "# Create the Hybrid training feature set\n",
        "X_hybrid_train = np.concatenate((Y_proba_rf_train, Y_proba_dnn_train), axis=1)\n",
        "Y_hybrid_train = Y_train\n",
        "print(f\"Hybrid Train Feature Shape: {X_hybrid_train.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Train the Meta-Classifier (Logistic Regression) ---\n",
        "meta_classifier = LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000, n_jobs=-1)\n",
        "\n",
        "print(\"\\nTraining Meta-Classifier (Logistic Regression)...\")\n",
        "meta_classifier.fit(X_hybrid_train, Y_hybrid_train)\n",
        "\n",
        "\n",
        "# --- Step 3: Create the testing feature set for the Meta-Classifier ---\n",
        "# Concatenate the test prediction probabilities created in the RF and DNN steps\n",
        "X_hybrid_test = np.concatenate((Y_proba_rf, Y_proba_dnn), axis=1)\n",
        "print(f\"Hybrid Test Feature Shape: {X_hybrid_test.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 4: Evaluate the Hybrid Model ---\n",
        "Y_pred_hybrid = meta_classifier.predict(X_hybrid_test)\n",
        "\n",
        "print(\"\\nHybrid Model Classification Report:\")\n",
        "print(classification_report(Y_test, Y_pred_hybrid))\n",
        "print(f\"Hybrid Model Accuracy: {accuracy_score(Y_test, Y_pred_hybrid):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, Y_pred_hybrid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a7dgu-HtbgZ",
        "outputId": "9cd57bfc-09f9-4225-8c13-6daa77fce17d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- 5. Training Hybrid (Stacked) Model ---\n",
            "\u001b[1m9899/9899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step\n",
            "Hybrid Train Feature Shape: (316753, 4)\n",
            "\n",
            "Training Meta-Classifier (Logistic Regression)...\n",
            "Hybrid Test Feature Shape: (79189, 4)\n",
            "\n",
            "Hybrid Model Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     53148\n",
            "           1       1.00      1.00      1.00     26041\n",
            "\n",
            "    accuracy                           1.00     79189\n",
            "   macro avg       1.00      1.00      1.00     79189\n",
            "weighted avg       1.00      1.00      1.00     79189\n",
            "\n",
            "Hybrid Model Accuracy: 0.9997\n",
            "\n",
            "Confusion Matrix:\n",
            "[[53146     2]\n",
            " [   21 26020]]\n"
          ]
        }
      ]
    }
  ]
}